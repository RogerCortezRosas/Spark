# -*- coding: utf-8 -*-
"""Transformaciones y acciones_Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ANPEHN6LQjXYoPR8kVxe0AKF-niwP1j
"""

from pyspark import SparkContext
from pyspark.sql import SparkSession

"""Un RDD (Resilient Distributed Dataset) es la estructura básica de datos en Spark.
Es una colección de datos distribuida en varios nodos de un cluster y permite realizar operaciones en paralelo.
"""

# Crea un contexto de Spark , para hacer la conexion entre mi codigo y el motor de Spark
#master = 'local' -> significa que spark se ejecutara en la maquina local usando solo nucleo de CPU 'local[*]' para usar todos los nucleos disponibles ,, En un entorno distribuido, podrías usar algo como 'yarn' o 'spark://<host>:<port>'.
# El parámetro appName define el nombre de la aplicación en Spark, útil para monitorear el trabajo en la interfaz web de Spark UI.
sc = SparkContext(master = 'local',appName =  'TransformacionesAcciones')

# Create a SparkSession
spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

data = [1,2,3,4]
rdd = sc.parallelize(data)

type(rdd)

rdd.collect()

sc

path = '/content/drive/MyDrive/Spark/files/'

"""# Obtencion csv con spark Cntext"""

# Obtyencion de archivo csv con spark Context
equiposOlimpicosRDD = sc.textFile(path + 'paises.csv').map(lambda line : line.split(","))

equiposOlimpicosRDD.take(10)

# ver cuantos paises participaron
equiposOlimpicosRDD.map(lambda x : (x[2])).distinct().count()# x[2] el elemnto 2 de cada lista

#Cuenta los rdd dentro de los primeros 20 milisegundos
equiposOlimpicosRDD.countApprox(20)

# ver los primeros elementos para ver el campo en comun entre equiposRDD y deportistaRDD
equiposOlimpicosRDD.show(2)

#Importamos los csv deportista y deportista2
deportistaRDD = sc.textFile(path + 'deportista.csv').map(lambda line : line.split(","))
deportista2RDD = sc.textFile(path + 'deportista2.csv').map(lambda line : line.split(","))

# Hacemos la union de los dos rdd
deportistaRDD = deportistaRDD.union(deportista2RDD)

#contamos la cantidad de rdds
deportistaRDD.count()

deportistaRDD.show()

"""## Obtener equipo y deportista"""

# es id de equiposRDD y equipo_id de deportistsRDD lo que los une
# Se selecciona la última columna del RDD (equipo_id) que es valor eje y el resto de contenidos
 # Se selecciona solo el id que es el valor eje y la sigla del país


deportistasEquipos = deportistaRDD.map(lambda line : [line[-1],line[:-1]]).join(equiposOlimpicosRDD.map(lambda x : [x[0],x[2]]))#.takeSample(False,6,25)  #False si quiero que no se repitan , 6 cuantos quiero en la salida  y 25 la semilla de aleatoriedad

lista_deportisata_equipo = deportistaRDD.map(lambda line : [line[-1],line[:-1]]).join(equiposOlimpicosRDD.map(lambda x : [x[0],x[2]])).top(10)

lista_deportisata_equipo[:]

lista_deportisata_equipo[1][0]

lista_deportisata_equipo[1][0][0]

deportistasEquipos.map(lambda x : (x[1][0][0],x[1][0][1:],x[1][1])).top(2)

# obtenemos info de resultado
resultado = sc.textFile(path + 'resultados.csv').map(lambda line : line.split(","))

resultado.take(7)

#Filtrar solo a los que ganaron medalla
resultado = resultado.filter(lambda l : 'NA' not in l[1])

resultado.take(3)

deportistaRDD.top(2)

equiposOlimpicosRDD.top(2)

# Juntamos el RDD de deportistas ,equiposcon el de resultado

# 1er parte juntamos deportistas con y sus equipos
deportistas_Equipos = deportistaRDD.map(lambda line : [line[-1],line[:-1]]).join(equiposOlimpicosRDD.map(lambda x : [x[0],x[2]]))

deportistas_Equipos.takeSample(False,6,25)

deportistas_Equipos.map(lambda x  : (x[1][0][0],(x[1][0][1:],x[1][1]))).take(2)

# 2da parte Juntamos con los resultados obteniendo las medallas de cada deportista haciendo la union con el deportista_id
deportistasGanadores = deportistas_Equipos.map(lambda x  : (x[1][0][0],(x[1][0][1:],x[1][1]))).join(resultado.map(lambda y : (y[2],y[1])))

deportistasGanadores.take(2)

"""Obtenemos los puntajes de cada pais de acuerdo al valor de las medallas"""

# Diccionario valores de las medallas
valoresMedallas = {'Gold':7,'Silver':5,'Bronze':4}

# primer parte se reduce la tupla
# x[1][0][1] -> Iniciales pais
# x[1][1] -> tipo de medalla
deportistasGanadores.map(lambda x : (x[1][0][1],x[1][1])).take(2)

# Segunda parte obtenemos los valores de cada medalla
deportistasGanadores.map(lambda x : (x[1][0][1],valoresMedallas[x[1][1]])).take(2)

# Guardamos en una  variable el rdd donde esta pais , puntos por medalla
paisesPuntos = deportistasGanadores.map(lambda x : (x[1][0][1],valoresMedallas[x[1][1]]))

paisesPuntos.take(2)

# hacemos la sumatoria por pais y ordenamos los resultados
from operator import add
conclusion = paisesPuntos.reduceByKey((add)).sortBy(lambda x : x[1],ascending = False)

conclusion.take(10)

"""# Obtencion de archivos csv con Spark Session"""

#Obtencion de archivo csv con spark session , cuando se hace la lectura con spak session se transforma en dataframe y con context en rdd
deportistaOlimpicoDF= spark.read.csv(path + 'deportista.csv',header = True)
deportistaOlimpico2DF= spark.read.csv(path + 'deportista2.csv',header = True)

# Union de los 2 Data Frame
DeportistaDF = deportistaOlimpicoDF.union(deportistaOlimpico2DF)

"""# Dataframes"""

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.storagelevel import StorageLevel
import pyspark.sql
from pyspark.sql import SQLContext
from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType
from pyspark.sql.types import Row

path = '/content/drive/MyDrive/Spark/files'

"""Creacion de schema"""

juegosSchema = StructType([StructField('juego_id',IntegerType(),False),# False si es obligatorio
                           StructField('nombre_juego',StringType(),False),
                           StructField("año",StringType(),False),
                           StructField("temporada",StringType(),False),
                           StructField("ciudad",StringType(),False)])

"""Lectura de schema"""

# Create a SparkSession
spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

"""Transformacion de RDD -> Df"""

# "header", "true" → Le dice a Spark que la primera fila del CSV contiene los nombres de las columnas.
juegoDF = spark.read.schema(juegosSchema).option ("header","true").csv(path + '/juegos.csv')

juegoDF.show(5)

spark

deportistaRDD.show()

type(deportistaRDD)

# Cambiar los tipos de datos del dataframe DeportistaDF
dict_tipos = {'deportista_id':IntegerType(),'genero':IntegerType(),'edad':IntegerType(),'altura':IntegerType(),'peso':FloatType(),'equipo_id':IntegerType()}

for col,tipo in dict_tipos.items():
  DeportistaDF = DeportistaDF.withColumn(col,DeportistaDF[col].cast(tipo))

DeportistaDF.dtypes

#Eliminamos columna Altura
DeportistaDF = deportistaRDD.drop('altura')

# Reasignamos nombre de columnas
DeportistaDF = DeportistaDF.withColumnRenamed('genero','sexo')

DeportistaDF.columns

# Uso de select
DeportistaDF.select('nombre','edad').show(5)

import pyspark.sql.functions as f

#Si usas col(), puedes encadenar métodos y hacer transformaciones más complejas,

DeportistaDF = DeportistaDF.select('deportista_id','nombre','sexo',f.col('edad').alias('edadAlJugar'),'equipo_id')

DeportistaDF

#Ordenar el Dataframe segun una columna
DeportistaDF.sort('edadAlJugar').show()

#Filtramos los deportistas que sean diferentes de cero
DeportistaDF = DeportistaDF.filter(DeportistaDF.edadAlJugar != 0)

DeportistaDF.sort('edadAlJugar').show()

#Obtencion de paises
paises = spark.read.csv(path + 'paises.csv',header=True)

paises.dtypes

paises = paises.withColumn('id',paises['id'].cast(IntegerType()))

paises.show(5)

# Obtencion de evento
eventoDF = spark.read.csv(path + 'evento.csv',header=True)

eventoDF.show(3)

eventoDF.dtypes

eventoDF = eventoDF.withColumn('evento_id',eventoDF['evento_id'].cast(IntegerType()))

#Obtewncion de resultados
resultados = spark.read.csv(path + 'resultados.csv',header=True)

resultados.dtypes

dict = {'resultado_id':IntegerType(),'medalla':StringType(),'deportista_id':IntegerType(),'juego_id':IntegerType(),'evento_id':IntegerType()}

for clave,valor in dict.items():
  resultados = resultados.withColumn(clave,resultados[clave].cast(valor))

resultados.dtypes

resultados.printSchema()

#Obtencion de juegos
juegosDF = spark.read.csv(path+'/juegos.csv',header=True)

#Renombramos y cambiamos tipo de datos
juegosDF = juegosDF.select(f.col('_c0').alias('juego_id').cast(IntegerType()),'nombre_juego','annio','temporada','ciudad')

juegosDF.show(5)

juegosDF = juegosDF.withColumn('annio',juegosDF['annio'].cast(IntegerType()))

juegosDF.dtypes

# Uso de joins para unir data frames
DeportistaDF.join( resultados,DeportistaDF.deportista_id == resultados.deportista_id , 'left')\
.join(juegosDF, juegosDF.juego_id == resultados.juego_id,'left')\
.join(eventoDF,eventoDF.evento_id == resultados.evento_id,'left')\
.select('nombre','edadAlJugar','medalla',f.col('annio').alias('año'),'evento').filter(resultados.medalla!= 'NA').show(5)

# union de medalla , pais y equipo que pertenecen
DeportistaDF.join(resultados,DeportistaDF.deportista_id == resultados.deportista_id,'left')\
.join(paises,paises.id == DeportistaDF.equipo_id,'left')\
.select('medalla','equipo','sigla').filter(resultados.medalla !='NA').show(5)

medallistaXAnio = DeportistaDF.join(resultados,DeportistaDF.deportista_id == resultados.deportista_id,'left')\
.join(juegosDF,juegosDF.juego_id == resultados.juego_id,'left')\
.join(paises,paises.id == DeportistaDF.equipo_id,'left')\
.join(eventoDF,eventoDF.evento_id == resultados.evento_id,'left')\
.select('sigla',f.col('annio').alias('año'),'medalla','equipo','nombre','evento')\
.filter(resultados.medalla !='NA')

medallista2XAnio = medallistaXAnio.groupBy('sigla','año').count()

medallista2XAnio.show()

medallista2XAnio.printSchema()

# Agrupamps por medallas totales y medallas promedio
medallista2XAnio.groupBy('sigla','año').agg(f.sum('count').alias('Total medallas'),f.avg('count').alias('Medallas promedio')).sort('año').show()

"""#SQL"""

resultados.registerTempTable('resultado')
DeportistaDF.registerTempTable('deportista')
paises.registerTempTable('paises')
juegosDF.registerTempTable('juegos')
eventoDF.registerTempTable('evento')

spark.sql("SELECT * FROM resultado").show(5)

#Requiere mas procesamiento al usar sql
spark.sql(""" SELECT medalla,equipo,sigla,j.annio FROM resultado r
              JOIN deportista d
              ON r.deportista_id = d.deportista_id
              JOIN paises p
              ON p.id = d.equipo_id
              JOIN juegos j
              ON j.juego_id = r.juego_id
              JOIN evento e
              ON e.evento_id = r.evento_id
              WHERE medalla <> "NA"
              GROUP BY medalla,equipo,sigla,j.annio
              ORDER BY annio ASC""").show()

"""# UDF
Las funciones definidas por el usuario o UDF, por sus siglas en inglés, son una funcionalidad agregada en Spark para definir funciones basadas en columnas las cuales permiten extender las capacidades de Spark al momento de transformar el set de datos.

Este tipo de implementaciones son convenientes cuando tenemos un desarrollo extenso donde hemos identificado la periodicidad de tareas repetitivas como suele ser en pasos de limpieza de datos, transformación o renombrado dinámico de columnas.

Por lo anterior es común encontrar en un proyecto de Spark una librería independiente donde existen todas estas funciones agregadas para que los desarrolladores involucrados en el proyecto puedan usarlas a conveniencia.

El uso de UDF no implica que las funciones que podemos crear nativamente con Python, Scala, R o Java no sean útiles. Una UDF tiene el objetivo de ofrecer un estándar interno en el proyecto que nos encontremos realizando. Además, en caso de ser necesario, una UDF puede ser modificada con ayuda de decoradores para que sea más extensible en diversos escenarios a los cuales nos podemos enfrentar.

Otro motivo para usar UDF es que en el módulo de Spark MLlib, la librería nativa de Spark para operaciones de Machine Learning, las UDF juegan un papel vital al momento de hacer transformaciones. Por lo cual tener un uso familiar de estas ampliará considerablemente la curva de aprendizaje de Spark MLlib.
"""

deportistaError = spark.read.csv(path + '/deportistaError.csv',header = True)

deportistaError.show()

deportistaError.dtypes

"""# Creacion de funcion UDF para transformar tipo de dato"""

from pyspark.sql.functions import udf

def conversionEnteros(valor):
  return int(valor) if valor !=None else None

conversionEnteros_udf = udf(lambda z : conversionEnteros(z),IntegerType())
#lambda z: conversionEnteros(z): Aplica la función conversionEnteros a cada valor de la columna.
spark.udf.register('conversionEnteros_udf', conversionEnteros_udf)
#Registra la UDF en el contexto SQL de Spark (SQLContext), lo que permite usarla en consultas SQL en Spark.

deportistaError.select(conversionEnteros_udf('altura').alias('alturaUDF')).show()

"""# particionado

Como se ha descrito en clases pasadas, los RDD son la capa de abstracción primaria para poder interactuar con los datos que viven en nuestro ambiente de Spark. Aunque estos puedan ser enmascarados con un esquema dotándolos de las facultades propias de los DataFrames, la información de fondo sigue operando como RDD.

Por lo tanto, la información, como indica el nombre de los RDD, se maneja de forma distribuida a lo largo del clúster, facilitando las operaciones que se van a ejecutar, ya que segmentos de información pueden encontrarse en diferentes ejecutores reduciendo el tiempo necesario para acceder a la información y poder así realizar los cálculos necesarios.

Cuando un RDD o Dataframe es creado, según las especificaciones que se indiquen a la aplicación de Spark, creará un esquema de particionado básico, el cual distribuirá los datos a lo largo del clúster. Siendo así que al momento de ejecutar una acción, esta se ejecutará entre los diversos fragmentos de información que existan para poder así realizar de la forma más rápida las operaciones. Es por eso que un correcto esquema de particionado es clave para poder tener aplicaciones rápidas y precisas que además consuman pocos recursos de red.

Otra de las tareas fundamentales es la replicación de componentes y sus fragmentos, ya que al aumentar la disponibilidad de estos podremos asegurar una tolerancia a fallos, mientras más se replique un valor es más probable que no se pierda si existe un fallo de red o energía, además de permitir una disponibilidad casi inmediata del archivo buscado.

La partición y replicación son elementos que deben ser analizados según el tipo de negocio o requerimientos que se tengan en el desarrollo que se encuentre en progreso, por lo cual la cantidad de datos replicados o granularidad de datos existentes en los fragmentos dependerá en función de las reglas de negocio.

"""



spark.stop()

sc.stop()



