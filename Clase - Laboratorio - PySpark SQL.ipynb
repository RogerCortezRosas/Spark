{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8276f39-ebfc-4f21-9ad1-399f84e5716c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2RJTjITkFCQG"
   },
   "source": [
    "**Spark SQL trabaja con DataFrames**. Un DataFrame, como ya lo hemos comentado es una **representación relacional de los datos**. Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf9553d-7368-480c-bc9e-1f369253c2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LlH6rSFoFCQK"
   },
   "source": [
    "### Creacion de un df desde 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c78ff-1d0b-4a0f-8d6a-de7fadffa2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "90Om8bW-FCQL"
   },
   "outputs": [],
   "source": [
    "emp = [(1, \"AAAAA\", \"dept1\", 1000),\n",
    "    (2, \"BBBBB\", \"dept1\", 1100),\n",
    "    (3, \"CCCCC\", \"dept1\", 2000),\n",
    "    (4, \"DDDDD\", \"dept1\", 3500),\n",
    "    (5, \"EEEEE\", \"dept2\", 8000),\n",
    "    (6, \"FFFFF\", \"dept2\", 5200),\n",
    "    (7, \"GGGGG\", \"dept3\", 3100),\n",
    "    (8, \"HHHHH\", \"dept3\", 6700),\n",
    "    (9, \"IIIII\", \"dept3\", 6500),\n",
    "    (10, \"JJJJJ\", \"dept4\", 5400)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "dfemp = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7eb607-0e60-40df-9bbd-43f314e3f73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eIrE4w6HFCQM",
    "outputId": "9e4fd174-24a5-4b3e-aa9e-4a43d15a68e0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n| id| name| dept|salary|\n+---+-----+-----+------+\n|  1|AAAAA|dept1|  1000|\n|  2|BBBBB|dept1|  1100|\n|  3|CCCCC|dept1|  2000|\n|  4|DDDDD|dept1|  3500|\n|  5|EEEEE|dept2|  8000|\n|  6|FFFFF|dept2|  5200|\n|  7|GGGGG|dept3|  3100|\n|  8|HHHHH|dept3|  6700|\n|  9|IIIII|dept3|  6500|\n| 10|JJJJJ|dept4|  5400|\n+---+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "dfemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82186e02-ed37-49be-a2e4-0354e39f69e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "owaw84fxFCQN",
    "outputId": "b2505910-eb49-4628-d476-f53ed97de7fc"
   },
   "outputs": [],
   "source": [
    "deptdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919e9be1-ff40-4a43-a804-eb8d5e03febe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EVZKDTK7FCQO"
   },
   "source": [
    "# Operaciones básicas en DataFrames\n",
    "\n",
    "Podemos aplicar las transformaciones que ya hemos visto en la seccion de RDDs, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9995564-d907-48f4-a275-a0fad5ffc676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZXuEgMQbFCQO",
    "outputId": "150d6bdb-dfa9-4d81-a323-887543abc677"
   },
   "outputs": [],
   "source": [
    "dfemp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be336061-0c5d-4427-9c6f-30975fed0b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TRy4nrbCFCQO",
    "outputId": "28c2fb04-9944-4515-fc37-53c3a3a1800e"
   },
   "outputs": [],
   "source": [
    "dfemp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd3b0f9-2601-474f-a323-0d85b53fc5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9S_66yomFCQP",
    "outputId": "1de4444f-57e6-42a2-d276-b31ac9476120"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1|AAAAA|\n|  2|BBBBB|\n|  3|CCCCC|\n|  4|DDDDD|\n|  5|EEEEE|\n|  6|FFFFF|\n|  7|GGGGG|\n|  8|HHHHH|\n|  9|IIIII|\n| 10|JJJJJ|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dfemp.select(\"id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf9a785-381b-4567-a97e-98cfdddee583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wYHiGY4tFCQP"
   },
   "source": [
    "### Ejemplo avanzado de: filter\n",
    "\n",
    "* Filtrar las filas según alguna condición.\n",
    "* Intentemos encontrar las filas con id = 1.\n",
    "* Hay diferentes formas de especificar la condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c6265d-5b9e-4928-a97c-896c8c58856c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hDnjmWboFCQP",
    "outputId": "f998648b-b6d6-40c2-bcac-749d5eaa8016"
   },
   "outputs": [],
   "source": [
    "dfemp.filter(dfemp[\"id\"] == 1).show()\n",
    "dfemp.filter(dfemp.id == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014b4e9b-08aa-433c-be74-a6ba431293c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "c-2FyTpHFCQQ",
    "outputId": "8301ba40-5ce7-4c69-dd14-423a28c968b2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n| id| name| dept|salary|\n+---+-----+-----+------+\n|  1|AAAAA|dept1|  1000|\n+---+-----+-----+------+\n\n+---+-----+-----+------+\n| id| name| dept|salary|\n+---+-----+-----+------+\n|  1|AAAAA|dept1|  1000|\n+---+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfemp.filter(col(\"id\") == 1).show()\n",
    "dfemp.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e65f304-aab9-4acc-93cb-9deed0e04075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DJSP03rAFCQQ"
   },
   "source": [
    "### Funcion: drop\n",
    "* Elimina una columna en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a55a0bcc-07c6-4281-8b39-077207b79dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QmbHEiG1FCQQ",
    "outputId": "1cb92df3-1e8a-487a-a348-dcdad53ceffc"
   },
   "outputs": [],
   "source": [
    "newdf = dfemp.drop(\"id\")\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd35de5-058f-4c33-9921-099de3baf942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Lp8MmGX7FCQQ"
   },
   "source": [
    "### Funcion: withColumn\n",
    "* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45c66f9-cf12-435b-b38f-df2f941d2283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4uRlk4FfFCQQ",
    "outputId": "cfaf0115-5344-4c11-de7b-488164e6bb67"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+-----+\n| id| name| dept|salary|bonus|\n+---+-----+-----+------+-----+\n|  1|AAAAA|dept1|  1000|100.0|\n|  2|BBBBB|dept1|  1100|110.0|\n|  3|CCCCC|dept1|  2000|200.0|\n|  4|DDDDD|dept1|  3500|350.0|\n|  5|EEEEE|dept2|  8000|800.0|\n|  6|FFFFF|dept2|  5200|520.0|\n|  7|GGGGG|dept3|  3100|310.0|\n|  8|HHHHH|dept3|  6700|670.0|\n|  9|IIIII|dept3|  6500|650.0|\n| 10|JJJJJ|dept4|  5400|540.0|\n+---+-----+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dfemp.withColumn(\"bonus\", col(\"salary\") * .1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6e0a420-796a-4be0-b97d-758dcb001282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OKs0NXARFCQQ"
   },
   "source": [
    "### Ejemplo de agregacion:\n",
    "* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39e12da-62df-4ebc-9924-4ccd9b4334f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6eGQiFXiFCQR",
    "outputId": "b448c7f6-bada-4d90-9743-ae5b7d00c5bc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+------+------+-----------------+\n| dept|conteo| suma|maximo|minimo|         promedio|\n+-----+------+-----+------+------+-----------------+\n|dept1|     4| 7600|  3500|  1000|           1900.0|\n|dept2|     2|13200|  8000|  5200|           6600.0|\n|dept3|     3|16300|  6700|  3100|5433.333333333333|\n|dept4|     1| 5400|  5400|  5400|           5400.0|\n+-----+------+-----+------+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "withColumn\n",
    "(dfemp.groupBy(\"dept\")\n",
    "    .agg(\n",
    "        f.count(\"salary\").alias(\"conteo\"),\n",
    "        f.sum(\"salary\").alias(\"suma\"),\n",
    "        f.max(\"salary\").alias(\"maximo\"),\n",
    "        f.min(\"salary\").alias(\"minimo\"),\n",
    "        f.avg(\"salary\").alias(\"promedio\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d35e94-da8c-4729-a403-ab5fdf540cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tSt7m29qFCQR"
   },
   "source": [
    "### Por ultimo, tambien podemos hacer joins, como en SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbfc359-72a3-4012-adff-577e41502168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KTm6sdR2FCQR",
    "outputId": "cf5508e1-f3d9-4448-d4b1-decb0e760c3f"
   },
   "outputs": [],
   "source": [
    "# Inner JOIN.\n",
    "dfemp.join(deptdf, dfemp[\"dept\"] == deptdf[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef13620-2801-4a10-9b30-cce026545201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZY_sLzKxFCQR"
   },
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1875bc64-e64e-42f6-ac71-83c4fcd5ffda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YJNMkHgvFCQR",
    "outputId": "89bb0517-ecfc-46ae-d798-ca25c4f4e645"
   },
   "outputs": [],
   "source": [
    "dfemp.join(deptdf, dfemp[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f606495f-831b-4671-ab7e-e0015a7b870e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VM-AfBZ-FCQR"
   },
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699de0d9-0219-41a9-9028-f732dfca5031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zsSadcVBFCQR",
    "outputId": "de8b1a02-4848-4ef6-b9b7-e46f9589b93b"
   },
   "outputs": [],
   "source": [
    "dfemp.join(deptdf, dfemp[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206e0307-2d6e-4e52-961b-69b9bdb2560b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wQlri817FCQR"
   },
   "source": [
    "### Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3875a9-4274-40a5-be8c-81dd4dc70f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "R_Yqy3JXFCQR",
    "outputId": "5fef3045-c271-4df1-fb26-09146736d78a"
   },
   "outputs": [],
   "source": [
    "dfemp.join(deptdf, dfemp[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f1e622-eb9f-4b6d-b194-de5541222176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Desde la librería \"pyspark.sql.types\" importamos los utilitarios \"StructType\" y el \"StructField\"\n",
    "#\"StrucType\" nos permite modificar el esquema de metadatos de un dataframe\n",
    "#\"StructField\" nos permite modificar a un campo del esquema de metadatos.\n",
    "#Tambien es necesario importar los tipos de datos que utilizaremos\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e008a2d9-36eb-43e4-a9d6-9c5df58d51f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leemos el archivo de persona , con un esquema de metadatos predefinido\n",
    "\n",
    "dfPersona = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").schema(\n",
    "    StructType(\n",
    "        [\n",
    "            StructField(\"ID\",StringType(),True),\n",
    "            StructField(\"NOMBRE\",StringType(),True),\n",
    "            StructField(\"TELEFONO\", StringType(), True),\n",
    "            StructField(\"CORREO\", StringType(), True),\n",
    "            StructField(\"FECHA_INGRESO\", StringType(), True),\n",
    "            StructField(\"EDAD\", IntegerType(), True),\n",
    "            StructField(\"SALARIO\", DoubleType(), True),\n",
    "            StructField(\"ID_EMPRESA\", StringType(), True)\n",
    "\n",
    "        ]\n",
    "    )\n",
    ").load(\"dbfs:/FileStore/shared_uploads/rcortezrosas@gmail.com/persona.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9ecdea-6b0e-4a6e-a3e8-6e4e72d26a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+--------------------+-------------+----+-------+----------+\n| ID|   NOMBRE|      TELEFONO|              CORREO|FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\n|  1|     Carl|1-745-633-9145|arcu.Sed.et@ante....|   2004-04-23|  32|20095.0|         5|\n|  2|Priscilla|      155-2498|Donec.egestas.Ali...|   2019-02-17|  34| 9298.0|         2|\n|  3|  Jocelyn|1-204-956-8594|amet.diam@loborti...|   2002-08-01|  27|10853.0|         3|\n|  4|    Aidan|1-719-862-9385|euismod.et.commod...|   2018-11-06|  29| 3387.0|        10|\n|  5|  Leandra|      839-8044|at@pretiumetrutru...|   2002-10-10|  41|22102.0|         1|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Mostramos los datos\n",
    "dfPersona.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f1d14c-2b6b-4782-8655-22cd3f1787be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creacion de la vista temporal para utilizar con Spark SQL\n",
    "dfPersona.createOrReplaceTempView('dfPersona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae7e813-ff8c-4d7f-8b06-e2448efc71da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+--------------------+-------------+----+-------+----------+\n| ID|   NOMBRE|      TELEFONO|              CORREO|FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\n|  1|     Carl|1-745-633-9145|arcu.Sed.et@ante....|   2004-04-23|  32|20095.0|         5|\n|  2|Priscilla|      155-2498|Donec.egestas.Ali...|   2019-02-17|  34| 9298.0|         2|\n|  5|  Leandra|      839-8044|at@pretiumetrutru...|   2002-10-10|  41|22102.0|         1|\n|  6|     Bert|      797-4453|a.felis.ullamcorp...|   2017-04-25|  70| 7800.0|         7|\n|  7|     Mark|1-680-102-6792|Quisque.ac@placer...|   2006-04-21|  52| 8112.0|         5|\n|  9|    Hanae|      935-2277|          eu@Nunc.ca|   2003-05-25|  69| 6834.0|         3|\n| 11|  Melyssa|      596-7736|vel@vulputateposu...|   2008-10-14|  48| 4913.0|         8|\n| 13|   Trevor|      512-1955|Nunc.quis.arcu@eg...|   2010-08-06|  34| 9501.0|         5|\n| 14|    Allen|      733-2795|felis.Donec@necle...|   2005-03-07|  59|16289.0|         2|\n| 17|     Omar|      720-1543|Phasellus.vitae.m...|   2014-06-24|  60| 6851.0|         6|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Filtramos alfunos regitros segun la edad\n",
    "spark.sql(\"SELECT * FROM dfPersona WHERE EDAD >30\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2441dd97-d6a9-406e-9359-2d14bdbe7d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+--------------------+-------------+----+-------+----------+\n| ID|   NOMBRE|      TELEFONO|              CORREO|FECHA_INGRESO|EDAD|SALARIO|ID_EMPRESA|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\n|  1|     Carl|1-745-633-9145|arcu.Sed.et@ante....|   2004-04-23|  32|20095.0|         5|\n|  2|Priscilla|      155-2498|Donec.egestas.Ali...|   2019-02-17|  34| 9298.0|         2|\n|  5|  Leandra|      839-8044|at@pretiumetrutru...|   2002-10-10|  41|22102.0|         1|\n|  6|     Bert|      797-4453|a.felis.ullamcorp...|   2017-04-25|  70| 7800.0|         7|\n|  7|     Mark|1-680-102-6792|Quisque.ac@placer...|   2006-04-21|  52| 8112.0|         5|\n|  9|    Hanae|      935-2277|          eu@Nunc.ca|   2003-05-25|  69| 6834.0|         3|\n| 11|  Melyssa|      596-7736|vel@vulputateposu...|   2008-10-14|  48| 4913.0|         8|\n| 13|   Trevor|      512-1955|Nunc.quis.arcu@eg...|   2010-08-06|  34| 9501.0|         5|\n| 14|    Allen|      733-2795|felis.Donec@necle...|   2005-03-07|  59|16289.0|         2|\n| 17|     Omar|      720-1543|Phasellus.vitae.m...|   2014-06-24|  60| 6851.0|         6|\n+---+---------+--------------+--------------------+-------------+----+-------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Usmos triple comilla para hacer comandos SQL con saltos de linea\n",
    "df1 = spark.sql(\"\"\"\n",
    "                SELECT *\n",
    "                FROM\n",
    "                dfPersona\n",
    "                WHERE\n",
    "                EDAD > 30 \"\"\")\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71773646-d8f2-4585-aaa7-8fc0b2e32016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parametrizacion de codigo SQL\n",
    "PARAM_EDAD = 30\n",
    "PARAM_SALARIO = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965b53d4-4903-4fa6-96b9-f18c23ea02ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+-------+\n| ID|   NOMBRE|EDAD|SALARIO|\n+---+---------+----+-------+\n|  1|     Carl|  32|20095.0|\n|  2|Priscilla|  34| 9298.0|\n|  5|  Leandra|  41|22102.0|\n|  6|     Bert|  70| 7800.0|\n|  7|     Mark|  52| 8112.0|\n+---+---------+----+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.sql(f\"\"\"SELECT\n",
    "                ID,NOMBRE,EDAD,SALARIO\n",
    "                FROM dfPersona\n",
    "                WHERE\n",
    "                EDAD >{PARAM_EDAD} AND \n",
    "                SALARIO > {PARAM_SALARIO}\"\"\")\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188f4fbe-01af-4f7e-8146-8cfcdbf93e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ejercicio 1 - Datos de Prueba\n",
    "# Realiar una consulta Utilizando Spark SQL para seleccionar el nombre y la edad de las personas cuya edad sea mayor a 28\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ejercicio1\").getOrCreate()\n",
    "\n",
    "# Definir el esquema de los datos\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"edad\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Crear un DataFrame de prueba\n",
    "data = [(1, \"Juan\", 25), (2, \"María\", 30), (3, \"Pedro\", 28)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Crear una vista temporal para utilizar en Spark SQL\n",
    "df.createOrReplaceTempView(\"personas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4915968a-2a84-4332-a455-5a981db304cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n| id|nombre|edad|\n+---+------+----+\n|  2| María|  30|\n+---+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#CON SPARK\n",
    "df_1 = df.filter(df['edad']>28)\n",
    "df_1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382f4a29-a1ba-4bc3-ac34-4d5e1e0c189e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|nombre|edad|\n+------+----+\n| María|  30|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "#Con SQL\n",
    "df_2 = spark.sql(\" SELECT nombre,edad FROM personas WHERE edad >28\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66cf9058-10f2-4395-a52e-f77caa4a5e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ejercicio 2 - Datos de prueba\n",
    "#Realiza una consulta utilizando Spark SQL para obtener la edad promedio de las personas en el DataFrame\n",
    "# Utilizamos el DataFrame creado en el ejercicio 1\n",
    "\n",
    "# Agregar más datos al DataFrame\n",
    "data_nuevos = [(4, \"Ana\", 22), (5, \"Luis\", 32), (6, \"Laura\", 27)]\n",
    "df_nuevos = spark.createDataFrame(data_nuevos, schema)\n",
    "\n",
    "# Union de DataFrames\n",
    "df = df.union(df_nuevos)\n",
    "# Crear una vista temporal para utilizar en Spark SQL\n",
    "df.createOrReplaceTempView(\"personas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08726752-9a83-4fe7-8d8a-9917dc4f695d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|         avg(edad)|\n+------------------+\n|27.333333333333332|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "(df.agg(f.avg('edad')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40fba910-7ad3-48f5-a1d8-42b13599f00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|         avg(edad)|\n+------------------+\n|27.333333333333332|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_3 = spark.sql(\" SELECT avg(edad) FROM personas\")\n",
    "df_3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e2add9-448c-4443-a5cf-a70287d19214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ejercicio  3 - Datos de prueba\n",
    "#Realizar una consulta de SPARK SQL para obtener el nombre , la edad y la direccion de las personas que tienen una direccion registrada\n",
    "# Utilizamos los DataFrames creados en los ejercicios anteriores\n",
    "\n",
    "# Crear un segundo DataFrame para realizar un join\n",
    "schema_direccion = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"direccion\", StringType(), True)\n",
    "])\n",
    "\n",
    "data_direccion = [(1, \"Calle A\"), (2, \"Calle B\"), (3, \"Calle C\"), (4, \"Calle D\")]\n",
    "df_direccion = spark.createDataFrame(data_direccion, schema_direccion)\n",
    "\n",
    "# Crear una vista temporal para el segundo DataFrame\n",
    "df_direccion.createOrReplaceTempView(\"direcciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85452884-684c-417b-a7b0-64f9ae5bd15e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n| id|nombre|edad|\n+---+------+----+\n|  1|  Juan|  25|\n|  2| María|  30|\n|  3| Pedro|  28|\n|  4|   Ana|  22|\n|  5|  Luis|  32|\n|  6| Laura|  27|\n+---+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" SELECT * FROM personas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf38c40-039d-42ba-8947-5525866ed67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n|nombre|edad|direccion|\n+------+----+---------+\n|  Juan|  25|  Calle A|\n| María|  30|  Calle B|\n| Pedro|  28|  Calle C|\n|   Ana|  22|  Calle D|\n+------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#con Spark\n",
    "\n",
    "df.join(df_direccion, df.id == df_direccion.id).select(\"nombre\", \"edad\", \"direccion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d17ba6-dc03-4520-add1-1d9ffbcba4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n|nombre|edad|direccion|\n+------+----+---------+\n|  Juan|  25|  Calle A|\n| María|  30|  Calle B|\n| Pedro|  28|  Calle C|\n|   Ana|  22|  Calle D|\n+------+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT nombre , edad,direccion \n",
    "          FROM personas p \n",
    "          RIGHT JOIN direcciones d ON d.id == p.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85bf6550-7923-4681-af12-eea8882be3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# UDF( User Defined Functions)\n",
    "\n",
    "¿Qué son las UDF en Apache Spark y por qué son importantes?\n",
    "Dentro del mundo de Apache Spark, las UDF (User Defined Functions) juegan un rol esencial al permitir a los usuarios definir sus propias funciones personalizadas que pueden ejecutarse de manera distribuida en todos los nodos de un clúster. Esto destaca porque Spark ya ofrece funciones nativas poderosas como map, filter y flatMap. Sin embargo, el poder registrar una UDF permite a los usuarios realizar transformaciones específicas que no son posibles con las funciones nativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc9cd437-ec7a-4ec9-b9b4-452b55d38a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n| id| name| dept|salary|\n+---+-----+-----+------+\n|  1|AAAAA|dept1|  1000|\n|  2|BBBBB|dept1|  1100|\n|  3|CCCCC|dept1|  2000|\n|  4|DDDDD|dept1|  3500|\n|  5|EEEEE|dept2|  8000|\n|  6|FFFFF|dept2|  5200|\n|  7|GGGGG|dept3|  3100|\n|  8|HHHHH|dept3|  6700|\n|  9|IIIII|dept3|  6500|\n| 10|JJJJJ|dept4|  5400|\n+---+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "dfemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992d51ac-323c-4453-9223-591df3b3a2e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detSalary_Level(salario):\n",
    "    level = None\n",
    "\n",
    "    if(salario > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(salario> 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(salario > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747fac61-438b-45be-b19a-269084aeca85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Registro de la funcion: detSalary_Level como UDF\n",
    "udf_detSalary_Level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05922c1-1cc5-443f-86bc-e0f9289ca594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+-------------+\n| id| name| dept|salary|nivel_salario|\n+---+-----+-----+------+-------------+\n|  1|AAAAA|dept1|  1000|   low_salary|\n|  2|BBBBB|dept1|  1100|   low_salary|\n|  3|CCCCC|dept1|  2000|   low_salary|\n|  4|DDDDD|dept1|  3500|   mid_salary|\n|  5|EEEEE|dept2|  8000|  high_salary|\n|  6|FFFFF|dept2|  5200|  high_salary|\n|  7|GGGGG|dept3|  3100|   mid_salary|\n|  8|HHHHH|dept3|  6700|  high_salary|\n|  9|IIIII|dept3|  6500|  high_salary|\n| 10|JJJJJ|dept4|  5400|  high_salary|\n+---+-----+-----+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Aplicacion de la udf\n",
    "newdf = dfemp.withColumn(\"nivel_salario\", udf_detSalary_Level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98b40a9-4b7e-46aa-a153-176f6397df1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ejercicio 1\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def cuadrado(num):\n",
    "    return num * num\n",
    "\n",
    "cuadrado_udf = udf(cuadrado, DoubleType())#Registras la funcion en UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ca247d-0a94-4be8-a335-74aa6fa99600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n| id| name| dept|salary|\n+---+-----+-----+------+\n|  1|AAAAA|dept1|  1000|\n|  2|BBBBB|dept1|  1100|\n|  3|CCCCC|dept1|  2000|\n|  4|DDDDD|dept1|  3500|\n|  5|EEEEE|dept2|  8000|\n|  6|FFFFF|dept2|  5200|\n|  7|GGGGG|dept3|  3100|\n|  8|HHHHH|dept3|  6700|\n|  9|IIIII|dept3|  6500|\n| 10|JJJJJ|dept4|  5400|\n+---+-----+-----+------+\n\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clase - Laboratorio - PySpark SQL",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
